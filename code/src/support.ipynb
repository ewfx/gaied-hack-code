{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd71a95e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting groq\n",
      "  Using cached groq-0.20.0-py3-none-any.whl.metadata (15 kB)\n",
      "Collecting anyio<5,>=3.5.0 (from groq)\n",
      "  Using cached anyio-4.9.0-py3-none-any.whl.metadata (4.7 kB)\n",
      "Collecting distro<2,>=1.7.0 (from groq)\n",
      "  Using cached distro-1.9.0-py3-none-any.whl.metadata (6.8 kB)\n",
      "Collecting httpx<1,>=0.23.0 (from groq)\n",
      "  Using cached httpx-0.28.1-py3-none-any.whl.metadata (7.1 kB)\n",
      "Collecting pydantic<3,>=1.9.0 (from groq)\n",
      "  Using cached pydantic-2.10.6-py3-none-any.whl.metadata (30 kB)\n",
      "Collecting sniffio (from groq)\n",
      "  Using cached sniffio-1.3.1-py3-none-any.whl.metadata (3.9 kB)\n",
      "Requirement already satisfied: typing-extensions<5,>=4.10 in d:\\github\\hackathon-code\\gaied-hack-code\\code\\.venv\\lib\\site-packages (from groq) (4.13.0)\n",
      "Requirement already satisfied: exceptiongroup>=1.0.2 in d:\\github\\hackathon-code\\gaied-hack-code\\code\\.venv\\lib\\site-packages (from anyio<5,>=3.5.0->groq) (1.2.2)\n",
      "Collecting idna>=2.8 (from anyio<5,>=3.5.0->groq)\n",
      "  Using cached idna-3.10-py3-none-any.whl.metadata (10 kB)\n",
      "Collecting certifi (from httpx<1,>=0.23.0->groq)\n",
      "  Using cached certifi-2025.1.31-py3-none-any.whl.metadata (2.5 kB)\n",
      "Collecting httpcore==1.* (from httpx<1,>=0.23.0->groq)\n",
      "  Using cached httpcore-1.0.7-py3-none-any.whl.metadata (21 kB)\n",
      "Collecting h11<0.15,>=0.13 (from httpcore==1.*->httpx<1,>=0.23.0->groq)\n",
      "  Using cached h11-0.14.0-py3-none-any.whl.metadata (8.2 kB)\n",
      "Collecting annotated-types>=0.6.0 (from pydantic<3,>=1.9.0->groq)\n",
      "  Using cached annotated_types-0.7.0-py3-none-any.whl.metadata (15 kB)\n",
      "Collecting pydantic-core==2.27.2 (from pydantic<3,>=1.9.0->groq)\n",
      "  Downloading pydantic_core-2.27.2-cp310-cp310-win_amd64.whl.metadata (6.7 kB)\n",
      "Using cached groq-0.20.0-py3-none-any.whl (124 kB)\n",
      "Using cached anyio-4.9.0-py3-none-any.whl (100 kB)\n",
      "Using cached distro-1.9.0-py3-none-any.whl (20 kB)\n",
      "Using cached httpx-0.28.1-py3-none-any.whl (73 kB)\n",
      "Using cached httpcore-1.0.7-py3-none-any.whl (78 kB)\n",
      "Using cached pydantic-2.10.6-py3-none-any.whl (431 kB)\n",
      "Downloading pydantic_core-2.27.2-cp310-cp310-win_amd64.whl (2.0 MB)\n",
      "   ---------------------------------------- 0.0/2.0 MB ? eta -:--:--\n",
      "   ---------- ----------------------------- 0.5/2.0 MB 2.8 MB/s eta 0:00:01\n",
      "   -------------------------- ------------- 1.3/2.0 MB 3.5 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 2.0/2.0 MB 3.1 MB/s eta 0:00:00\n",
      "Using cached sniffio-1.3.1-py3-none-any.whl (10 kB)\n",
      "Using cached annotated_types-0.7.0-py3-none-any.whl (13 kB)\n",
      "Using cached idna-3.10-py3-none-any.whl (70 kB)\n",
      "Using cached certifi-2025.1.31-py3-none-any.whl (166 kB)\n",
      "Using cached h11-0.14.0-py3-none-any.whl (58 kB)\n",
      "Installing collected packages: sniffio, pydantic-core, idna, h11, distro, certifi, annotated-types, pydantic, httpcore, anyio, httpx, groq\n",
      "Successfully installed annotated-types-0.7.0 anyio-4.9.0 certifi-2025.1.31 distro-1.9.0 groq-0.20.0 h11-0.14.0 httpcore-1.0.7 httpx-0.28.1 idna-3.10 pydantic-2.10.6 pydantic-core-2.27.2 sniffio-1.3.1\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "# %pip install groq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4196cbb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "system_prompt = \"\"\"You are an banking support assitant. You will be provided with the following inputs:  \n",
    "1. An **email** or other relevant **document** related to banking operations.  \n",
    "2. A set of **rules** (guidelines) defining data extraction, classification criteria, and processing logic.  \n",
    "3. A predefined list of **request type definitions** relevant to the domain.\n",
    "\n",
    "*Output Requirements:*  \n",
    "Based on the input, your task is to generate the following outputs:  \n",
    "1. **Request Types with Prioritization and Confidence Scores:**  \n",
    "   - Determine the possible request types from the input.  \n",
    "   - Assign a priority level to each request type (e.g., High, Medium, Low).  \n",
    "   - Provide a confidence score (in percentage) for each request type based on your assessment.  \n",
    "\n",
    "2. **Extracted Field Values:**  \n",
    "   - Identify and extract all relevant field values mentioned in the input (e.g., customer ID, account details, loan type, date, amount).  \n",
    "   - Ensure the extracted fields comply with the provided rules and are consistent across documents.  \n",
    "\n",
    "3. **Duplicate Detection Fields with Classification Flags:**  \n",
    "   - Analyze the content for potential duplicate records or information.  \n",
    "   - If a duplicate is detected, classify it as a \"Duplicate\" and provide a detailed reason (e.g., matching request ID, similar account details, etc.).  \n",
    "   - Include a flag to indicate the reason for classification.\n",
    "\n",
    "4.**Based on the provided email content, generate a service ticket in JSON format. The ticket must include the following fields:**\n",
    "\n",
    "   - ticket_id: A unique identifier for the ticket, prefixed with 'TICKET-' followed by the email file name (without the extension).\n",
    "   - request_types: A list of request types, each with a type, priority (High, Medium, Low), and confidence (percentage).\n",
    "   - fields: A dictionary of extracted fields such as account numbers, references, and other relevant details.\n",
    "   - duplicates: A list of duplicate detection results, if any, with reasons for classification.\n",
    "   Ensure the output is a valid JSON object and does not include any additional text, notes, or explanations outside the JSON object.\"   \n",
    "\n",
    "   \n",
    "Print json output in the following format: \n",
    "{ \n",
    "  \"ticket_id\": \"TICKET-<file_name>\", \n",
    "  \"request_types\": [ \n",
    "    { \n",
    "      \"type\": \"<request_type>\", \n",
    "      \"priority\": \"<High|Medium|Low>\", \n",
    "      \"confidence\": <percentage> \n",
    "    } \n",
    "  ], \n",
    "  \"fields\": { \n",
    "    \"<field_name>\": \"<field_value>\" \n",
    "  }, \n",
    "  \"duplicates\": [ \n",
    "    { \n",
    "      \"duplicate\": true, \n",
    "      \"reason\": \"<reason>\" \n",
    "    } \n",
    "  ] \n",
    "}  \n",
    "Note:Don't output the email body back.\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8d2f335c",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'groq'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[5], line 5\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01memail\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m policy\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01memail\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mparser\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m BytesParser\n\u001b[1;32m----> 5\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mgroq\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Groq\n\u001b[0;32m      7\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mparse_eml_standard\u001b[39m(eml_content):\n\u001b[0;32m      8\u001b[0m     msg \u001b[38;5;241m=\u001b[39m BytesParser(policy\u001b[38;5;241m=\u001b[39mpolicy\u001b[38;5;241m.\u001b[39mdefault)\u001b[38;5;241m.\u001b[39mparsebytes(eml_content)\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'groq'"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from email import policy\n",
    "from email.parser import BytesParser\n",
    "\n",
    "from groq import Groq\n",
    "\n",
    "def parse_eml_standard(eml_content):\n",
    "    msg = BytesParser(policy=policy.default).parsebytes(eml_content)\n",
    "    from_ = msg['From']\n",
    "    to = msg['To']\n",
    "    cc = msg['Cc']\n",
    "    body = msg.get_body(preferencelist=('plain')).get_content()\n",
    "    return from_, to, cc, body\n",
    "\n",
    "def extract_email_attributes_standard(eml_folder):\n",
    "    email_tuples = []\n",
    "    total_emails = len([f for f in os.listdir(eml_folder) if f.endswith('.eml')])\n",
    "    for index, eml_file in enumerate(os.listdir(eml_folder)):\n",
    "        if eml_file.endswith('.eml'):\n",
    "            with open(os.path.join(eml_folder, eml_file), 'rb') as file:\n",
    "                from_, to, cc, body = parse_eml_standard(file.read())\n",
    "                email_tuples.append((from_, to, cc, body, eml_file, index))\n",
    "                process_email_attributes(from_, to, cc, body, eml_file, index, total_emails)\n",
    "    return email_tuples\n",
    "\n",
    "# Set to store hashes of processed emails\n",
    "processed_hashes = set()\n",
    "\n",
    "def generate_email_hash(from_, to, cc, body):\n",
    "    \"\"\"\n",
    "    Generate a unique hash for an email based on its content.\n",
    "    \"\"\"\n",
    "    email_content = f\"{from_}{to}{cc}{body}\"\n",
    "    return hashlib.md5(email_content.encode('utf-8')).hexdigest()\n",
    "\n",
    "\n",
    "def process_email_attributes(from_, to, cc, body, eml_file, index, total_emails):\n",
    "    \"\"\"\n",
    "    Process email attributes and avoid processing duplicates.\n",
    "    \"\"\"\n",
    "    global processed_hashes\n",
    "\n",
    "    # Generate a hash for the current email\n",
    "    email_hash = generate_email_hash(from_, to, cc, body)\n",
    "\n",
    "    # Check if the email is a duplicate\n",
    "    if email_hash in processed_hashes:\n",
    "        print(f\"Skipping duplicate email: {eml_file}\")\n",
    "        return\n",
    "\n",
    "    # Add the hash to the set of processed emails\n",
    "    processed_hashes.add(email_hash)\n",
    "\n",
    "    # Process the email\n",
    "   # print(f\"# Processing email {index + 1}/{total_emails}: {eml_file}\")\n",
    "    print(classify_email_with_groq(body))\n",
    "    \n",
    "\n",
    "def classify_email_with_groq(body):\n",
    "    client = Groq(api_key=os.environ.get(\"GROQ_API_KEY\"))\n",
    "    classification = client.chat.completions.create(\n",
    "        messages=[\n",
    "            {\n",
    "                \"role\": \"system\",\n",
    "                \"content\": system_prompt\n",
    "            },\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": body\n",
    "            }\n",
    "        ],\n",
    "        model=\"llama3-8b-8192\"\n",
    "    )\n",
    "    return classification.choices[0].message.content.strip()\n",
    "\n",
    "# Example usage\n",
    "eml_folder = './emails'  # Replace with the path to your EML files\n",
    "email_tuples_standard = extract_email_attributes_standard(eml_folder)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
